---
title: "Homework 2 Coding Questions"
author: "Su Goh"
date: "1/11/2020"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Question 3

### 3.1 
```{r}
x1 <- c(4, 3, 5, 7)
y <- c(6, 2, 4, 11)
summary(lm(y ~ x1))
```

The intercept has a coefficient of -3.8857, and \(X_1\) has a coefficient of 2.0286. The p-value of the t-statistics for both of the coefficients is greater than \(\alpha=0.05\), so we do not reject the null hypothesis that they are both equal to 0. In other words, \(X_1\) does not predict Y. 

### 3.2
``` {r}
X <- array(c(rep(1, 4), x1), dim=c(4,2))
XtX <- t(X) %*% X
XtX_inv <- solve(XtX)
(beta <- (XtX_inv %*% t(X) %*% y))
```

\(\textbf{X}^T\textbf{X} = \begin{pmatrix} 4 & 9 \\ 19 & 99 \end{pmatrix}\), 
\((\textbf{X}^T\textbf{X})^{-1} = \begin{pmatrix} 2.8286 & -0.5429 \\ -0.5429 & 0.1143 \end{pmatrix}\), \(\beta = (\textbf{X}^T\textbf{X})^{-1}(\textbf{X}^T\textbf{Y}) = 
  \begin{pmatrix} -3.8857 \\ 2.0286\end{pmatrix}\\\)

This is the same result as the one generated by the lm function.

### 3.3
```{r}
res <- residuals(lm(y ~ x1))
ssres <- sum(res^2)
(sigma2 <- ssres / (4-2))
```

\(\hat{\sigma^2} = 4.371429\)


### 3.4

```{r}
(var_beta <- sigma2 * XtX_inv)
```

\(\hat{Var(\hat{\beta}}) = \begin{pmatrix} 12.3649 & -2.3731 \\ -2.3731 & 0.4996\end{pmatrix}\)

From the variance-covariance matrix, we have that: \(\hat{Var(\hat{\beta}}_0) = 12.3649\), \(\hat{Var(\hat{\beta}}_1) = 0.4996\) and \(Cov(\hat{\beta_0}, \hat{\beta_1}) = -2.3731\).


### 3.5
```{r}
sqrt(var_beta[1,1])
sqrt(var_beta[2,2])
```

\[ese(\hat{\beta_0}) = 3.5164\]
\[ese(\hat{\beta_1}) = 0.7068\]


## Question 4
### 4.1

```{r}
file1 <-"http://www.math.mcgill.ca/yyang/regression/data/salary.csv"
salary <- read.csv(file1, header=TRUE)
x1 <- salary$SPENDING/1000
y <- salary$SALARY
fit.Salary <- lm(y ~ x1)

n <- length(x1) 
X <- array(c(rep(1, n), x1), dim=c(n,2))
(beta <- (solve(t(X) %*% X) %*% t(X) %*% y))
```

These estimates are the same as the one in the lm summary table.

### 4.2
```{r}
yhat <- X %*% beta
e <- y - yhat
sum(e)
xbar <- rep(mean(x1), length(x1))
(t(x1-xbar) %*% e)
(t(yhat) %*% e)
```

They all approximate to 0, the residuals are orthogonal to the predictors.

### 4.3
We can compute the expected standard error of \(\hat{\beta_0}\) with the following:

\[T_0 = \frac{\hat{\beta_0}}{ese(\hat{\beta_0})} \implies ese(\hat{\beta_0}) = \frac{\hat{\beta_0}}{T_0}\]
\[ese(\hat{\beta_0}) = \sqrt{\hat{\sigma}^2(\frac{1}{n} + \frac{\bar{x_1}^2}{S_{xx}})} \]


```{r}
12129.4/10.13

sigma2 <- sum(e^2)/(n-2)
sxx <- sum((x1-xbar)^2)
(ese0 <- sqrt(sigma2 * (1/51 + mean(x1)^2/sxx)))
```

### 4.4
```{r}
(rse <- sqrt(sigma2))
```


### 4.5

The model we have is: \(\text{salary} = \beta_0 + \beta_1 \cdot \text{spending}\).

To test if there is a linear relationship between spending (X1) and salary (Y), we test the following hypothesis:

\[ H_0: \beta_1 = 0\]
\[H_1: \beta_1 \neq 0 \]

If we fail to reject \(H_0\), this implies that there is no linear relationship between spending and salary.

Let \(\alpha = 0.05\). Thus, we reject \(H_0\) if the p-value of the t-statistic, \(T_1 = \frac{\hat{\beta_1}}{ese(\hat{\beta_1})} \sim t_{n-2}\), is less than 0.05. 

```{r}
(t1 <- beta[2,1]/311.7)
qt(c(0.025, 0.975), df=n-2)
2*pt(-abs(t1), df=n-2)
```

Since \(T_1 = 10.61144 > 2.009575\), the t-statistic falls within the rejection region. Furthermore, the p-value is very small (\(p=2.705595e^{-14}\)). Thus we can reject the null hypothesis to conclude that there is a linear relationship between spending and salary.


### 4.6
We can construct a confidence interval for \(\beta_1\): \(CI(\beta_1) = [\hat{\beta_1} - k \cdot ese(\hat{\beta_1}), \hat{\beta_1} + k \cdot ese(\hat{\beta_1})]\) where \(k = t_{\frac{\alpha}{2}, n-2}\).

```{r}
k <- qt(0.995,df=n-2)
(ci <- c(beta[2,1]-(k*311.7), beta[2,1]+(k*311.7)))

c <- 3500
(t1 <- (beta[2,1]-c)/311.7)
2*pt(-abs(t1), df=n-2)
```

The 99% confidence interval is [2472.244, 4142.926]. This means that \(\beta_1\) is contained in that interval with probability = 0.99. 

3500 lies in the 99% confidence interval. For this value, the hypothesis being tested is: 

\[H_0: \beta_1 = 3500 \]
\[H_1: \beta_1 \neq 3500\]

Since 3500 falls within the 99%CI, we do not reject the null hypothesis that \(\beta_1 = 3500\). 

The t-statistic is -0.6173083 and the p-value is 0.5399. This is greater than \(\alpha=0.01\) and so we fail to reject the null hypothesis.

We conclude that we cannot rule out that the true value for \(\beta_1\) is 3500.


### 4.7
As in *4.5*, we are testing if there is a linear relationship between spending (X1) and salary (Y). The hypothesis is:
\[ H_0: Y= \beta_0 + \epsilon \]
\[ H_1: Y = \beta_0 + \beta_1 X_1 + \epsilon \]
which is equivalent to
\[ H_0: \beta_1 = 0 \]
\[ H_1: \beta_1 \neq 0 \]

```{r}
ssres <- sum(e^2) 
ssr <- sum((yhat-mean(y))^2)
(f <- (ssr)/(ssres/(n-2)))
pf(f, 1, (n-2), lower.tail = FALSE)
```

\(F=112.5995\) and the corresponding p-value is \(2.707e^{-14}\). At \(\alpha=0.01\) we can reject the null hypothesis to conclude that there is indeed a linear relationship between spending and salary. 

### 4.8
```{r}
(sst <- sum((y-mean(y))^2))
(ssr + ssres)
```
The result that \(SS_T = SS_R + SS_{Res}\) is true.

### 4.9
```{r}
predict(fit.Salary, data.frame(x1=5))
```

The average public teacher annual salary when public spending is \$5000 will be \$28667.30.

### 4.10
```{r}
Xnew <- t(c(1, 5))
Hnew <- diag(Xnew %*% solve(t(X) %*% X) %*% t(Xnew))
var_ynew <- sigma2 * Hnew
(ese_new <- sqrt(var_ynew))
```

### 4.11
Prediction interval is given by: \([\hat{Y} - k \cdot ese(\hat{Y}),  \hat{Y} + k \cdot ese(\hat{Y})]\).

```{r}
ynew <- Xnew %*% beta
alpha <- 0.05
k <- qt(1-alpha/2, n-2)

var_y0 <- sigma2 * (1 + Hnew)
ese_y0 <- sqrt(var_y0)
(conf.vals <- cbind(ynew - k * ese_new, ynew + k * ese_new))
predict(fit.Salary, data.frame(x1=5), interval='confidence')

(pred.vals <- cbind(ynew - k * ese_y0, ynew + k * ese_y0))
predict(fit.Salary, data.frame(x1=5), interval='prediction')
```


## Question 5
### 5.1
```{r}
data.source <-"http://www.math.mcgill.ca/yyang/regression/data/genetic_data.csv"
genetic_data <- read.csv(file=data.source)
y <- genetic_data$y
g1 <- genetic_data$X14046
linmod <- lm(y ~ g1)
t_original <- summary(linmod)$coef[2,3]

permutation_test <- function(y, x, M) {
  tstat_vec <- rep(0, M)
  for (i in 1:M) {
    y_permuted <- sample(y)
    model_permuted <- lm(y_permuted ~ x)
    tstat_vec[i] <- summary(model_permuted)$coef[2, 3]
  }
  return(tstat_vec)
}

pvalue <- function(t_original, tstat_vec, M){
  p <- sum(tstat_vec > abs(t_original) | tstat_vec < -abs(t_original))/M
  return(p)
}
M = 1000
tstat_vec <- permutation_test(y, g1, M)
(p <- pvalue(t_original, tstat_vec, M))
```

As the p-value is not less than \(\alpha=0.05\), we do not reject the null hypothesis (\(H_0: \beta_1 = 0\)). Thus, we conclude that there is no evidence of a statistically significant linear relationship between gene \(g_1\) and the syndrome.


### 5.2, 5.3

```{r}
all_pvalues <- rep(-1, 10)
all_pvalues_t <- rep(-1, 10)

for(i in 1:10){
  gi <- genetic_data[,i+1]
  t_gi <- summary(lm(y~gi))$coef[2,3]
  tstat_gi <- permutation_test(y, gi, M)
  all_pvalues[i] <- pvalue(t_gi, tstat_gi, M)
  all_pvalues_t[i] <- summary(lm(y~gi))$coef[2,4]
}
t(all_pvalues)
```
Genes \(g_{6,7,8,9,10}\) are all strongly correlated with the syndrome. We cannot tell which has the strongest correlation as their p-values are all approximately 0. 

### 5.4 
```{r}
sprintf('%.6f', all_pvalues_t)
```
The same result is seen if we take the p-values from the t-tests conducted for each \(g_i\) for \(i=1,2, ..., 10\).