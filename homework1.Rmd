---
title: "Homework 1 Coding Questions"
author: "Su Goh"
date: "04/10/2020"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# graphing set up
addTrans <- function(color,trans) {
  # This function adds transparancy to a color.
  # Define transparancy with an integer between 0 and 255
  # 0 being fully transparant and 255 being fully visable
  # Works with either color and trans a vector of equal length,
  # or one of the two of length 1.
  if (length(color)!=length(trans)&!any(c(length(color),length(trans))==1)){ 
	  stop("Vector lengths not correct")
  }
	if (length(color)==1 & length(trans)>1) color <- rep(color,length(trans)) 
	if (length(trans)==1 & length(color)>1) trans <- rep(trans,length(color))

  num2hex <- function(x) {
	  	hex <- unlist(strsplit("0123456789ABCDEF",split=""))
		return(paste(hex[(x-x%%16)/16+1],hex[x%%16+1],sep="")) 
	}

	rgb <- rbind(col2rgb(color),trans)
	res <- paste("#",apply(apply(rgb,2,num2hex),2,paste,collapse=""),sep="") 
	return(res)
}
```

## Question 6
### a) Simulation problem
First, I set up dataframe for the 100 sampled observations of X and Y. Xi's follow a uniform distribution ranging from -1 to 1, and the errors follow a standard normal distribution. 

```{r 6a}
linmod.norm <- function(n, beta.0, beta.1) {
  x <- runif(n, min=-1, max=1)  # X ~ U(-1, 1)
  epsilon <- rnorm(n, 0, 1)  # e ~ N(0, 1)
  y <- beta.0 + beta.1*x + epsilon  # model specification
  return(data.frame(x=x, y=y))
}

df = linmod.norm(n=100, beta.0=5, beta.1=3)
plot(x=df$x,y=df$y, xlab='X', ylab='Y')
abline(a=5, b=3, lwd = 3, col = addTrans("red",200))
```


### b) Repeated sampling
I repeat the experiment in a 1000 times to obtain the estimate \(\hat{\beta_1}\).
```{r 6b}
beta1.est = vector(length=1000)
for(i in 1:1000){
  df = linmod.norm(n=100, beta.0=5, beta.1=3)
  beta1.est[i] = coefficients(lm(y ~ x, data = df))[2]
}

beta1.mean = mean(beta1.est)
comp = 'equal to'
if (beta1.mean < 3){
  comp = 'less than'
} else if (beta1.mean > 3){
  comp = 'greater than'
} 
sprintf('The mean of these values is %f, %s beta1 = 3.', beta1.mean, comp)
hist(beta1.est, xlab='Estimated beta1')
```


### c) Errors follow Cauchy distribution
```{r 6c}

linmod.cauchy <- function(n, beta.0, beta.1) {
  x <- runif(n, min=-1, max=1)  # X ~ U(-1, 1)
  epsilon <- rcauchy(n)  # e ~ cauchy
  y <- beta.0 + beta.1*x + epsilon  # model specification
  return(data.frame(x=x, y=y))
}

beta1.cauchy = vector(length=1000)
for(i in 1:1000){
  df.cauchy = linmod.cauchy(n=100, beta.0=5, beta.1=3)
  beta1.cauchy[i] = coefficients(lm(y ~ x, data = df.cauchy))[2]
}

beta1.mean = mean(beta1.cauchy)
comp = 'equal to'
if (beta1.mean < 3){
  comp = 'less than'
} else if (beta1.mean > 3){
  comp = 'greater than'
} 
sprintf('The mean of these values is %f, %s beta1 = 3.', beta1.mean, comp)

hist(beta1.cauchy)
```

The range of the histogram has increased significantly. It is no longer centered around 3 closely. The Cauchy distribution has fatter tails than the normal distribution, hence the \(\hat{\beta_1}\) ends up taking on varied and spread out values. 


### d) Xs with errors
```{r 6d}
linmod.norm.error <- function(n, beta.0, beta.1) {
  x <- runif(n, min=-1, max=1)  # X ~ U(-1, 1)
  delta <- rnorm(n, 0, 2)
  epsilon <- rnorm(n, 0, 1)  # e ~ N(0, 1)
  y <- beta.0 + beta.1*x + epsilon  # model specification
  return(data.frame(x=x, w=x+delta, y=y))
}

df.error = linmod.norm.error(n=100, beta.0=5, beta.1=3)
plot(x=df.error$w,y=df.error$y, xlab='W', ylab='Y')
abline(a=5, b=3, lwd = 3, col = addTrans("red",200))  #true regression line
abline(lm(y ~ w, data=df.error))  # fitted regression line
```


```{r 6d_repeat}

beta1.errors = vector(length=1000)
for(i in 1:1000){
  df.error = linmod.norm.error(n=100, beta.0=5, beta.1=3)
  beta1.errors[i] = coefficients(lm(y ~ w, data = df.error))[2]
}

beta1.mean = mean(beta1.errors)
comp = 'equal to'
if (beta1.mean < 3){
  comp = 'less than'
} else if (beta1.mean > 3){
  comp = 'greater than'
} 
sprintf('The mean of these values is %f, %s beta1 = 3.', beta1.mean, comp)
hist(beta1.errors)
```

The mean for the \(\hat{\beta_1}\) in the model where X is observed with errors (i.e. W) is significantly lower than in the model in part a. The smaller coefficient leads us to believe that the variable X has far less of an effect on Y than it truly does.

## Question 7
```{r 7 load data}
library("readr")
kidney <- read_delim("http://www.math.mcgill.ca/yyang/regression/data/kidney.txt"," ",
                     escape_double = FALSE, trim_ws = TRUE)
x_kidney = kidney$age
y_kidney = kidney$tot
```

### a) 
```{r}
plot(x_kidney, y_kidney, xlab='Age', ylab='Total kidney function')
abline(lm(y_kidney ~ x_kidney))
```

The relationship between \(age\) and \(tot\) is not linear. Plotting the linear relationship based on the LS model doesn't offer a very good fit, as we can see there are significant number of datapoints that are quite distant from the line. 


### b) Test set
```{r 7b test}
age_grid = data.frame(x_test = seq(min(x_kidney), max(x_kidney), by=0.2))
```

### c) KNN prediction
```{r 7c knn}
library(FNN)

pred_k1 = knn.reg(train=x_kidney, test=age_grid, y=y_kidney, k=1)
pred_k5 = knn.reg(train=x_kidney, test=age_grid, y=y_kidney, k=5)
pred_k10 = knn.reg(train=x_kidney, test=age_grid, y=y_kidney, k=10)
pred_k25 = knn.reg(train=x_kidney, test=age_grid, y=y_kidney, k=25)
pred_k50 = knn.reg(train=x_kidney, test=age_grid, y=y_kidney, k=50)
pred_k157 = knn.reg(train=x_kidney, test=age_grid, y=y_kidney, k=157)

par(mfrow = c(3,2))

plot(y_kidney ~ x_kidney,  xlab='Age', ylab='Total kidney function', main="k=1")
lines(x = age_grid$x_test, y = pred_k1$pred, col='orange')

plot(y_kidney ~ x_kidney,  xlab='Age', ylab='Total kidney function', main="k=5")
lines(x = age_grid$x_test, y = pred_k5$pred, col='orange')

plot(y_kidney ~ x_kidney,  xlab='Age', ylab='Total kidney function', main="k=10")
lines(x = age_grid$x_test, y = pred_k10$pred, col='orange')

plot(y_kidney ~ x_kidney,  xlab='Age', ylab='Total kidney function', main="k=25")
lines(x = age_grid$x_test, y = pred_k25$pred, col='orange')

plot(y_kidney ~ x_kidney,  xlab='Age', ylab='Total kidney function', main="k=50")
lines(x = age_grid$x_test, y = pred_k50$pred, col='orange')

plot(y_kidney ~ x_kidney,  xlab='Age', ylab='Total kidney function', main="k=157")
lines(x = age_grid$x_test, y = pred_k157$pred, col='orange')
```

Which k value is the best? I choose the one which results in the smallest root MSE.

```{r 7c mse}
sqrt(mean((y_kidney - pred_k1$pred)^2))
sqrt(mean((y_kidney - pred_k5$pred)^2))
sqrt(mean((y_kidney - pred_k10$pred)^2))
sqrt(mean((y_kidney - pred_k25$pred)^2))
sqrt(mean((y_kidney - pred_k50$pred)^2))
sqrt(mean((y_kidney - pred_k157$pred)^2))
```

I think there's something wrong with my code implementation but I am stuck on it so just going to submit what I have an explain my reasoning. :( According to the computed RMSE, when \(k=157\), the RMSE is the smallest. However, looking at the plots tells us that the model is very overfitted, the line is almost horizontal but the data points clearly are not linear. If I had to choose, I would say k=25 has the best fit as it is rather smooth but still has the general shape that matches the distribution of the data points. 


### d) Test set [0, 18]
```{r 7d}
underage_grid = data.frame(x_test = seq(0,18, by=0.1))

pred_k1 = knn.reg(train=x_kidney, test=underage_grid, y=y_kidney, k=1)
pred_k5 = knn.reg(train=x_kidney, test=underage_grid, y=y_kidney, k=5)
pred_k10 = knn.reg(train=x_kidney, test=underage_grid, y=y_kidney, k=10)
pred_k25 = knn.reg(train=x_kidney, test=underage_grid, y=y_kidney, k=25)
pred_k50 = knn.reg(train=x_kidney, test=underage_grid, y=y_kidney, k=50)
pred_k157 = knn.reg(train=x_kidney, test=underage_grid, y=y_kidney, k=157)

par(mfrow = c(3,2))

plot(y_kidney ~ x_kidney,  xlab='Age', ylab='Total kidney function', main="k=1", xlim=c(0,120))
lines(x = underage_grid$x_test, y = pred_k1$pred, col='blue')

plot(y_kidney ~ x_kidney,  xlab='Age', ylab='Total kidney function', main="k=5", xlim=c(0,120))
lines(x = underage_grid$x_test, y = pred_k5$pred, col='blue')

plot(y_kidney ~ x_kidney,  xlab='Age', ylab='Total kidney function', main="k=10", xlim=c(0,120))
lines(x = underage_grid$x_test, y = pred_k10$pred, col='blue')

plot(y_kidney ~ x_kidney,  xlab='Age', ylab='Total kidney function', main="k=25", xlim=c(0,120))
lines(x = underage_grid$x_test, y = pred_k25$pred, col='blue')

plot(y_kidney ~ x_kidney,  xlab='Age', ylab='Total kidney function', main="k=50", xlim=c(0,120))
lines(x = underage_grid$x_test, y = pred_k50$pred, col='blue')

plot(y_kidney ~ x_kidney,  xlab='Age', ylab='Total kidney function', main="k=157", xlim=c(0,120))
lines(x = underage_grid$x_test, y = pred_k157$pred, col='blue')
```

The predictions tend towards 0 as k increases and the predicted relationship appears to be completely linear. This is because the range of the test set is outside of the given data range, so there is no basis for prediction.


### e) Test set [88, 120]
```{r 7e}
overage_grid = data.frame(x_test = seq(88,120, by=0.1))

pred_k1 = knn.reg(train=x_kidney, test=overage_grid, y=y_kidney, k=1)
pred_k5 = knn.reg(train=x_kidney, test=overage_grid, y=y_kidney, k=5)
pred_k10 = knn.reg(train=x_kidney, test=overage_grid, y=y_kidney, k=10)
pred_k25 = knn.reg(train=x_kidney, test=overage_grid, y=y_kidney, k=25)
pred_k50 = knn.reg(train=x_kidney, test=overage_grid, y=y_kidney, k=50)
pred_k157 = knn.reg(train=x_kidney, test=overage_grid, y=y_kidney, k=157)

par(mfrow = c(3,2))

plot(y_kidney ~ x_kidney,  xlab='Age', ylab='Total kidney function', main="k=1", xlim=c(0,120))
lines(x = overage_grid$x_test, y = pred_k1$pred, col='green')

plot(y_kidney ~ x_kidney,  xlab='Age', ylab='Total kidney function', main="k=5", xlim=c(0,120))
lines(x = overage_grid$x_test, y = pred_k5$pred, col='green')

plot(y_kidney ~ x_kidney,  xlab='Age', ylab='Total kidney function', main="k=10", xlim=c(0,120))
lines(x = overage_grid$x_test, y = pred_k10$pred, col='green')

plot(y_kidney ~ x_kidney,  xlab='Age', ylab='Total kidney function', main="k=25", xlim=c(0,120))
lines(x = overage_grid$x_test, y = pred_k25$pred, col='green')

plot(y_kidney ~ x_kidney,  xlab='Age', ylab='Total kidney function', main="k=50", xlim=c(0,120))
lines(x = overage_grid$x_test, y = pred_k50$pred, col='green')

plot(y_kidney ~ x_kidney,  xlab='Age', ylab='Total kidney function', main="k=157", xlim=c(0,120))
lines(x = overage_grid$x_test, y = pred_k157$pred, col='green')
```

This faces the same problem as 7d, where the test data is outside of the range of the given data. Like 7d, the horizontal line tends to 0 as k increases and the predicted relationship is linear. 
