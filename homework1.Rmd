---
title: "Homework 1 Coding Questions"
author: "Su Goh"
date: "04/10/2020"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# graphing set up
addTrans <- function(color,trans) {
  # This function adds transparancy to a color.
  # Define transparancy with an integer between 0 and 255
  # 0 being fully transparant and 255 being fully visable
  # Works with either color and trans a vector of equal length,
  # or one of the two of length 1.
  if (length(color)!=length(trans)&!any(c(length(color),length(trans))==1)){ 
	  stop("Vector lengths not correct")
  }
	if (length(color)==1 & length(trans)>1) color <- rep(color,length(trans)) 
	if (length(trans)==1 & length(color)>1) trans <- rep(trans,length(color))

  num2hex <- function(x) {
	  	hex <- unlist(strsplit("0123456789ABCDEF",split=""))
		return(paste(hex[(x-x%%16)/16+1],hex[x%%16+1],sep="")) 
	}

	rgb <- rbind(col2rgb(color),trans)
	res <- paste("#",apply(apply(rgb,2,num2hex),2,paste,collapse=""),sep="") 
	return(res)
}
```

## Question 6
### a) Simulation problem
First, I set up dataframe for the 100 sampled observations of X and Y. Xi's follow a uniform distribution ranging from -1 to 1, and the errors follow a standard normal distribution. 

```{r 6a}
set.seed(1)

linmod.norm <- function(n, beta.0, beta.1) {
  x <- runif(n, min=-1, max=1)  # X ~ U(-1, 1)
  epsilon <- rnorm(n, 0, 1)  # e ~ N(0, 1)
  y <- beta.0 + beta.1*x + epsilon  # model specification
  return(data.frame(x=x, y=y))
}

df = linmod.norm(n=100, beta.0=5, beta.1=3)
plot(x=df$x,y=df$y, xlab='X', ylab='Y')
fit = lm(df$y ~ df$x)
abline(fit,col='red')
```


### b) Repeated sampling
I repeat the experiment in a 1000 times to obtain the estimate \(\hat{\beta_1}\).
```{r 6b}
set.seed(1)

beta1.est = vector(length=1000)
for(i in 1:1000){
  df = linmod.norm(n=100, beta.0=5, beta.1=3)
  beta1.est[i] = coefficients(lm(y ~ x, data = df))[2]
}

beta1.mean = mean(beta1.est)
comp = 'equal to'
if (beta1.mean < 3){
  comp = 'less than'
} else if (beta1.mean > 3){
  comp = 'greater than'
} 
sprintf('The mean of these values is %f, %s beta1 = 3.', beta1.mean, comp)
hist(beta1.est, breaks=50, xlab='Estimated beta1')
```


### c) Errors follow Cauchy distribution
```{r 6c}
set.seed(1)

n <- 100
betas <- rep(0, 1000)
for(i in 1:1000) {
X <- runif(n, -1, 1)
epsilon <- rcauchy(n, 0, 1)
Y <- 5 + 3*X + epsilon
fit = lm(Y ~ X)
betas[i] <- fit$coef[2]
}
mean(betas)

hist(beta1.cauchy, breaks=500, xlim = c(-50,50))
```

The range of the histogram has increased significantly. It is no longer centered around 3 closely. The Cauchy distribution has fatter tails than the normal distribution, hence the \(\hat{\beta_1}\) ends up taking on varied and spread out values. 


### d) Xs with errors
```{r 6d}
set.seed(12345)
linmod.norm.error <- function(n, beta.0, beta.1) {
  x <- runif(n, min=-1, max=1)  # X ~ U(-1, 1)
  delta <- rnorm(n, 0, sqrt(2))  # note that in rnorm it takes sd and not var!
  epsilon <- rnorm(n, 0, 1)  # e ~ N(0, 1)
  y <- beta.0 + beta.1*x + epsilon  # model specification
  return(data.frame(x=x, w=x+delta, y=y))
}

df.error = linmod.norm.error(n=100, beta.0=5, beta.1=3)
plot(x=df.error$w,y=df.error$y, xlab='W', ylab='Y')
abline(a=5, b=3, lwd = 3, col = addTrans("red",200))  #true regression line
abline(lm(y ~ w, data=df.error))  # fitted regression line
```


```{r 6d_repeat}

beta1.errors = vector(length=1000)
for(i in 1:1000){
  df.error = linmod.norm.error(n=100, beta.0=5, beta.1=3)
  beta1.errors[i] = coefficients(lm(y ~ w, data = df.error))[2]
}

beta1.mean = mean(beta1.errors)
comp = 'equal to'
if (beta1.mean < 3){
  comp = 'less than'
} else if (beta1.mean > 3){
  comp = 'greater than'
} 
sprintf('The mean of these values is %f, %s beta1 = 3.', beta1.mean, comp)
hist(beta1.errors)
```

The mean for the \(\hat{\beta_1}\) in the model where X is observed with errors (i.e. W) is significantly lower than in the model in part a, so it is biased. The smaller coefficient leads us to believe that the variable X has far less of an effect on Y than it truly does.

## Question 7
```{r 7 load data}
library("readr")
kidney <- read_delim("http://www.math.mcgill.ca/yyang/regression/data/kidney.txt"," ",
                     escape_double = FALSE, trim_ws = TRUE)
x_kidney = kidney$age
y_kidney = kidney$tot
```

### a) 
```{r}
plot(x_kidney, y_kidney, xlab='Age', ylab='Total kidney function')
abline(lm(y_kidney ~ x_kidney))
```

There is a negative linear relationship between \(age\) and \(tot\).


### b) Test set
```{r 7b test}
age_grid = data.frame(x_test = seq(min(kidney$age), max(x_kidney), by=0.01))
```

### c) KNN prediction
```{r 7c knn}
library(FNN)

pred_k1 = knn.reg(train=x_kidney, test=age_grid, y=y_kidney, k=1)
pred_k5 = knn.reg(train=x_kidney, test=age_grid, y=y_kidney, k=5)
pred_k10 = knn.reg(train=x_kidney, test=age_grid, y=y_kidney, k=10)
pred_k25 = knn.reg(train=x_kidney, test=age_grid, y=y_kidney, k=25)
pred_k50 = knn.reg(train=x_kidney, test=age_grid, y=y_kidney, k=50)
pred_k157 = knn.reg(train=x_kidney, test=age_grid, y=y_kidney, k=157)

plot(y_kidney ~ x_kidney,  xlab='Age', ylab='Total kidney function', main="k=1")
lines(x = age_grid$x_test, y = pred_k1$pred, col='orange')

plot(y_kidney ~ x_kidney,  xlab='Age', ylab='Total kidney function', main="k=5")
lines(x = age_grid$x_test, y = pred_k5$pred, col='orange')

plot(y_kidney ~ x_kidney,  xlab='Age', ylab='Total kidney function', main="k=10")
lines(x = age_grid$x_test, y = pred_k10$pred, col='orange')

plot(y_kidney ~ x_kidney,  xlab='Age', ylab='Total kidney function', main="k=25")
lines(x = age_grid$x_test, y = pred_k25$pred, col='orange')

plot(y_kidney ~ x_kidney,  xlab='Age', ylab='Total kidney function', main="k=50")
lines(x = age_grid$x_test, y = pred_k50$pred, col='orange')

plot(y_kidney ~ x_kidney,  xlab='Age', ylab='Total kidney function', main="k=157")
lines(x = age_grid$x_test, y = pred_k157$pred, col='orange')
```

 k=25 has the best fit as it is rather smooth but still has the general shape that matches the distribution of the data points. 


### d) Test set [0, 18]
```{r 7d}
underage_grid = data.frame(x_test = seq(0,18, by=0.1))

pred_k1 = knn.reg(train=x_kidney, test=underage_grid, y=y_kidney, k=1)
pred_k5 = knn.reg(train=x_kidney, test=underage_grid, y=y_kidney, k=5)
pred_k10 = knn.reg(train=x_kidney, test=underage_grid, y=y_kidney, k=10)
pred_k25 = knn.reg(train=x_kidney, test=underage_grid, y=y_kidney, k=25)
pred_k50 = knn.reg(train=x_kidney, test=underage_grid, y=y_kidney, k=50)
pred_k157 = knn.reg(train=x_kidney, test=underage_grid, y=y_kidney, k=157)

par(mfrow = c(3,2))

plot(y_kidney ~ x_kidney,  xlab='Age', ylab='Total kidney function', main="k=1", xlim=c(0,120))
lines(x = underage_grid$x_test, y = pred_k1$pred, col='blue')

plot(y_kidney ~ x_kidney,  xlab='Age', ylab='Total kidney function', main="k=5", xlim=c(0,120))
lines(x = underage_grid$x_test, y = pred_k5$pred, col='blue')

plot(y_kidney ~ x_kidney,  xlab='Age', ylab='Total kidney function', main="k=10", xlim=c(0,120))
lines(x = underage_grid$x_test, y = pred_k10$pred, col='blue')

plot(y_kidney ~ x_kidney,  xlab='Age', ylab='Total kidney function', main="k=25", xlim=c(0,120))
lines(x = underage_grid$x_test, y = pred_k25$pred, col='blue')

plot(y_kidney ~ x_kidney,  xlab='Age', ylab='Total kidney function', main="k=50", xlim=c(0,120))
lines(x = underage_grid$x_test, y = pred_k50$pred, col='blue')

plot(y_kidney ~ x_kidney,  xlab='Age', ylab='Total kidney function', main="k=157", xlim=c(0,120))
lines(x = underage_grid$x_test, y = pred_k157$pred, col='blue')
```

The predictions tend towards 0 as k increases and the line of prediction is on the left on the training set. This is because the range of the test set is outside of the given data range, so there is no basis for prediction.


### e) Test set [88, 120]
```{r 7e}
overage_grid = data.frame(x_test = seq(88,120, by=0.1))

pred_k1 = knn.reg(train=x_kidney, test=overage_grid, y=y_kidney, k=1)
pred_k5 = knn.reg(train=x_kidney, test=overage_grid, y=y_kidney, k=5)
pred_k10 = knn.reg(train=x_kidney, test=overage_grid, y=y_kidney, k=10)
pred_k25 = knn.reg(train=x_kidney, test=overage_grid, y=y_kidney, k=25)
pred_k50 = knn.reg(train=x_kidney, test=overage_grid, y=y_kidney, k=50)
pred_k157 = knn.reg(train=x_kidney, test=overage_grid, y=y_kidney, k=157)

par(mfrow = c(3,2))

plot(y_kidney ~ x_kidney,  xlab='Age', ylab='Total kidney function', main="k=1", xlim=c(0,120))
lines(x = overage_grid$x_test, y = pred_k1$pred, col='green')

plot(y_kidney ~ x_kidney,  xlab='Age', ylab='Total kidney function', main="k=5", xlim=c(0,120))
lines(x = overage_grid$x_test, y = pred_k5$pred, col='green')

plot(y_kidney ~ x_kidney,  xlab='Age', ylab='Total kidney function', main="k=10", xlim=c(0,120))
lines(x = overage_grid$x_test, y = pred_k10$pred, col='green')

plot(y_kidney ~ x_kidney,  xlab='Age', ylab='Total kidney function', main="k=25", xlim=c(0,120))
lines(x = overage_grid$x_test, y = pred_k25$pred, col='green')

plot(y_kidney ~ x_kidney,  xlab='Age', ylab='Total kidney function', main="k=50", xlim=c(0,120))
lines(x = overage_grid$x_test, y = pred_k50$pred, col='green')

plot(y_kidney ~ x_kidney,  xlab='Age', ylab='Total kidney function', main="k=157", xlim=c(0,120))
lines(x = overage_grid$x_test, y = pred_k157$pred, col='green')
```

This faces the same problem as 7d, where the test data is outside of the range of the given data. Like 7d, the horizontal line is to the right of the training set and tends to 0 as k increases and the predicted relationship is linear. 
