---
title: "Homework 2 Coding Questions"
author: "Su Goh"
date: "1/11/2020"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Question 3

### 3.1 
```{r}
x1 <- c(4, 3, 5, 7)
y <- c(6, 2, 4, 11)
summary(lm(y ~ x1))
```

The intercept has a coefficient of -3.8857, and \(X_1\) has a coefficient of 2.0286. The p-value of the t-statistics for both of the coefficients is greater than \(\alpha=0.05\), so we do not reject the null hypothesis that they are both equal to 0. In other words, \(X_1\) does not predict Y. 

### 3.2
``` {r}
X <- array(c(rep(1, 4), x1), dim=c(4,2))
XtX <- t(X) %*% X
XtX_inv <- solve(XtX)
(beta <- (XtX_inv %*% t(X) %*% y))
```

\(\textbf{X}^T\textbf{X} = \begin{pmatrix} 4 & 9 \\ 19 & 99 \end{pmatrix} \)
\((\textbf{X}^T\textbf{X})^{-1} = \begin{pmatrix} 2.8286 & -0.5429 \\ -0.5429 & 0.1143 \end{pmatrix}\)
\(\beta = (\textbf{X}^T\textbf{X})^{-1}(\textbf{X}^T\textbf{Y}) = 
  \begin{pmatrix} -3.8857 \\ 2.0286\end{pmatrix}\)

This is the same result as the one generated by the lm function.

### 3.3
```{r}
res <- residuals(lm(y ~ x1))
ssres <- sum(res^2)
(sigma2 <- ssres / (4-2))
```

\(\hat{\sigma^2} = 4.371429\)


### 3.4

```{r}
(var_beta <- sigma2 * XtX_inv)
```

\(\hat{Var(\hat{\beta}}) = \begin{pmatrix} 12.3649 & -2.3731 \\ -2.3731 & 0.4996\end{pmatrix}\)

From the variance-covariance matrix, we have that:

\(\hat{Var(\hat{\beta}}_0) = 12.3649\), \(\hat{Var(\hat{\beta}}_1) = 0.4996\) and \(Cov(\hat{\beta_0}, \hat{\beta_1}) = -2.3731\).


### 3.5
```{r}
sqrt(var_beta[1,1])
sqrt(var_beta[2,2])
```

\(ese(\hat{\beta_0}) = 3.5164\)
\(ese(\hat{\beta_1}) = 0.7068\)

## Question 4

```{r}
file1 <-"http://www.math.mcgill.ca/yyang/regression/data/salary.csv"
salary <- read.csv(file1, header=TRUE)
x1 <- salary$SPENDING/1000
y <- salary$SALARY
fit.Salary <- lm(y ~ x1)
summary(fit.Salary)
```
### 4.1

```{r}
n <- length(x1) 
X <- array(c(rep(1, n), x1), dim=c(n,2))
(beta <- (solve(t(X) %*% X) %*% t(X) %*% y))
```

These estimates are the same as the one in the lm summary table.

### 4.2
```{r}
e <- y - (X %*% beta)
sum(e)
xbar <- rep(mean(x1), length(x1))
(t(x1-xbar) %*% e)
(t(X %*% beta) %*% e)
```

They all approximate to 0, the residuals are orthogonal to the predictors.

### 4.3
We can compute the expected standard error of \(\hat{\beta_0}\) with the following:

\(T_0 = \frac{\hat{\beta_0}}{ese(\hat{\beta_0})} \implies ese(\hat{\beta_0}) = \frac{\hat{\beta_0}}{T_0}\)


\(ese(\hat{\beta_0}) = \sqrt{\hat{\sigma}^2(\frac{1}{n} + \frac{\bar{x_1}^2}{S_{xx}})} \)


```{r}
12129.4/10.13

sigma2 <- sum(e^2)/(51-2)
sxx <- sum((x1-xbar)^2)
(ese0 <- sqrt(sigma2 * (1/51 + mean(x1)^2/sxx)))
```

### 4.4
```{r}
(rse <- sqrt(sigma2))
```

### 4.5


